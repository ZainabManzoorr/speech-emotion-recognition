{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0593890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8481d3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The government announced new policies to boost the technology sector.\n",
      "Stock markets reacted positively to the increase in foreign investments.\n",
      "Researchers developed a new AI model for medical diagnosis.\n",
      "Climate change continues to affect weather patterns worldwide.\n",
      "Startups are increasingly adopting machine learning solutions.\n",
      "Cybersecurity threats have increased with digital transformation.\n",
      "Education systems are integrating artificial intelligence tools.\n",
      "Healthcare analytics is improving patient treatment outcomes.\n",
      "Renewable energy investments are growing rapidly.\n",
      "Global economies are adapting to automation and AI technologies.\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\"The government announced new policies to boost the technology sector.\n",
    "Stock markets reacted positively to the increase in foreign investments.\n",
    "Researchers developed a new AI model for medical diagnosis.\n",
    "Climate change continues to affect weather patterns worldwide.\n",
    "Startups are increasingly adopting machine learning solutions.\n",
    "Cybersecurity threats have increased with digital transformation.\n",
    "Education systems are integrating artificial intelligence tools.\n",
    "Healthcare analytics is improving patient treatment outcomes.\n",
    "Renewable energy investments are growing rapidly.\n",
    "Global economies are adapting to automation and AI technologies.\"\"\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "951b0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents =sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a86dd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35cee7f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The government announced new policies to boost the technology sector.\n",
      "Stock markets reacted positively to the increase in foreign investments.\n",
      "Researchers developed a new AI model for medical diagnosis.\n",
      "Climate change continues to affect weather patterns worldwide.\n",
      "Startups are increasingly adopting machine learning solutions.\n",
      "Cybersecurity threats have increased with digital transformation.\n",
      "Education systems are integrating artificial intelligence tools.\n",
      "Healthcare analytics is improving patient treatment outcomes.\n",
      "Renewable energy investments are growing rapidly.\n",
      "Global economies are adapting to automation and AI technologies.\n"
     ]
    }
   ],
   "source": [
    "for sentnece in documents:\n",
    "  print(sentnece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "626374ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39eba023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'government',\n",
       " 'announced',\n",
       " 'new',\n",
       " 'policies',\n",
       " 'to',\n",
       " 'boost',\n",
       " 'the',\n",
       " 'technology',\n",
       " 'sector',\n",
       " '.',\n",
       " 'Stock',\n",
       " 'markets',\n",
       " 'reacted',\n",
       " 'positively',\n",
       " 'to',\n",
       " 'the',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'foreign',\n",
       " 'investments',\n",
       " '.',\n",
       " 'Researchers',\n",
       " 'developed',\n",
       " 'a',\n",
       " 'new',\n",
       " 'AI',\n",
       " 'model',\n",
       " 'for',\n",
       " 'medical',\n",
       " 'diagnosis',\n",
       " '.',\n",
       " 'Climate',\n",
       " 'change',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'affect',\n",
       " 'weather',\n",
       " 'patterns',\n",
       " 'worldwide',\n",
       " '.',\n",
       " 'Startups',\n",
       " 'are',\n",
       " 'increasingly',\n",
       " 'adopting',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'solutions',\n",
       " '.',\n",
       " 'Cybersecurity',\n",
       " 'threats',\n",
       " 'have',\n",
       " 'increased',\n",
       " 'with',\n",
       " 'digital',\n",
       " 'transformation',\n",
       " '.',\n",
       " 'Education',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'integrating',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'tools',\n",
       " '.',\n",
       " 'Healthcare',\n",
       " 'analytics',\n",
       " 'is',\n",
       " 'improving',\n",
       " 'patient',\n",
       " 'treatment',\n",
       " 'outcomes',\n",
       " '.',\n",
       " 'Renewable',\n",
       " 'energy',\n",
       " 'investments',\n",
       " 'are',\n",
       " 'growing',\n",
       " 'rapidly',\n",
       " '.',\n",
       " 'Global',\n",
       " 'economies',\n",
       " 'are',\n",
       " 'adapting',\n",
       " 'to',\n",
       " 'automation',\n",
       " 'and',\n",
       " 'AI',\n",
       " 'technologies',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "568f5725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dcb2a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'government',\n",
       " 'announced',\n",
       " 'new',\n",
       " 'policies',\n",
       " 'to',\n",
       " 'boost',\n",
       " 'the',\n",
       " 'technology',\n",
       " 'sector',\n",
       " '.',\n",
       " 'Stock',\n",
       " 'markets',\n",
       " 'reacted',\n",
       " 'positively',\n",
       " 'to',\n",
       " 'the',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'foreign',\n",
       " 'investments',\n",
       " '.',\n",
       " 'Researchers',\n",
       " 'developed',\n",
       " 'a',\n",
       " 'new',\n",
       " 'AI',\n",
       " 'model',\n",
       " 'for',\n",
       " 'medical',\n",
       " 'diagnosis',\n",
       " '.',\n",
       " 'Climate',\n",
       " 'change',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'affect',\n",
       " 'weather',\n",
       " 'patterns',\n",
       " 'worldwide',\n",
       " '.',\n",
       " 'Startups',\n",
       " 'are',\n",
       " 'increasingly',\n",
       " 'adopting',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'solutions',\n",
       " '.',\n",
       " 'Cybersecurity',\n",
       " 'threats',\n",
       " 'have',\n",
       " 'increased',\n",
       " 'with',\n",
       " 'digital',\n",
       " 'transformation',\n",
       " '.',\n",
       " 'Education',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'integrating',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'tools',\n",
       " '.',\n",
       " 'Healthcare',\n",
       " 'analytics',\n",
       " 'is',\n",
       " 'improving',\n",
       " 'patient',\n",
       " 'treatment',\n",
       " 'outcomes',\n",
       " '.',\n",
       " 'Renewable',\n",
       " 'energy',\n",
       " 'investments',\n",
       " 'are',\n",
       " 'growing',\n",
       " 'rapidly',\n",
       " '.',\n",
       " 'Global',\n",
       " 'economies',\n",
       " 'are',\n",
       " 'adapting',\n",
       " 'to',\n",
       " 'automation',\n",
       " 'and',\n",
       " 'AI',\n",
       " 'technologies',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6574d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import  TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f9de18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'government',\n",
       " 'announced',\n",
       " 'new',\n",
       " 'policies',\n",
       " 'to',\n",
       " 'boost',\n",
       " 'the',\n",
       " 'technology',\n",
       " 'sector.',\n",
       " 'Stock',\n",
       " 'markets',\n",
       " 'reacted',\n",
       " 'positively',\n",
       " 'to',\n",
       " 'the',\n",
       " 'increase',\n",
       " 'in',\n",
       " 'foreign',\n",
       " 'investments.',\n",
       " 'Researchers',\n",
       " 'developed',\n",
       " 'a',\n",
       " 'new',\n",
       " 'AI',\n",
       " 'model',\n",
       " 'for',\n",
       " 'medical',\n",
       " 'diagnosis.',\n",
       " 'Climate',\n",
       " 'change',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'affect',\n",
       " 'weather',\n",
       " 'patterns',\n",
       " 'worldwide.',\n",
       " 'Startups',\n",
       " 'are',\n",
       " 'increasingly',\n",
       " 'adopting',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'solutions.',\n",
       " 'Cybersecurity',\n",
       " 'threats',\n",
       " 'have',\n",
       " 'increased',\n",
       " 'with',\n",
       " 'digital',\n",
       " 'transformation.',\n",
       " 'Education',\n",
       " 'systems',\n",
       " 'are',\n",
       " 'integrating',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'tools.',\n",
       " 'Healthcare',\n",
       " 'analytics',\n",
       " 'is',\n",
       " 'improving',\n",
       " 'patient',\n",
       " 'treatment',\n",
       " 'outcomes.',\n",
       " 'Renewable',\n",
       " 'energy',\n",
       " 'investments',\n",
       " 'are',\n",
       " 'growing',\n",
       " 'rapidly.',\n",
       " 'Global',\n",
       " 'economies',\n",
       " 'are',\n",
       " 'adapting',\n",
       " 'to',\n",
       " 'automation',\n",
       " 'and',\n",
       " 'AI',\n",
       " 'technologies',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecb08a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> run\n",
      "runs --> run\n",
      "runner --> runner\n",
      "ran --> ran\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "#PorterStemming\n",
    "words = [\"running\", \"runs\", \"runner\", \"ran\", \"easily\", \"fairly\"]\n",
    "from nltk.stem import PorterStemmer\n",
    "stemming = PorterStemmer()\n",
    "for word in words:\n",
    "  print(f\"{word} --> {stemming.stem(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb1c310f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> runn\n",
      "runs --> run\n",
      "runner --> runner\n",
      "ran --> ran\n",
      "easily --> easily\n",
      "fairly --> fairly\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
    "for word in words:\n",
    "  print(f\"{word} --> {reg_stemmer.stem(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79d84101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> run\n",
      "runs --> run\n",
      "runner --> runner\n",
      "ran --> ran\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "#SnowBall Stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "for word in words:\n",
    "  print(f\"{word} --> {snow_stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebd1bad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> running\n",
      "runs --> run\n",
      "runner --> runner\n",
      "ran --> ran\n",
      "easily --> easily\n",
      "fairly --> fairly\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in words:\n",
    "  print(f\"{word} --> {lemmatizer.lemmatize(word,pos='n')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33e5ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "993b06e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b39fc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ENVY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ENVY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet,stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7db2cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f862259a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "  words = word_tokenize(sentences[i])\n",
    "  words = [SnowballStemmer.stem(word) for word in words if word not in set(stopwords)]\n",
    "  sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "622b9d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the govern announc new polici boost technolog sector .',\n",
       " 'stock market react posit increas foreign invest .',\n",
       " 'research develop new ai model medic diagnosi .',\n",
       " 'climat chang continu affect weather pattern worldwid .',\n",
       " 'startup increas adopt machin learn solut .',\n",
       " 'cybersecur threat increas digit transform .',\n",
       " 'educ system integr artifici intellig tool .',\n",
       " 'healthcar analyt improv patient treatment outcom .',\n",
       " 'renew energi invest grow rapid .',\n",
       " 'global economi adapt autom ai technolog .']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29441797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8be0f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "  words = nltk.word_tokenize(sentences[i])\n",
    "  words = [lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords)]\n",
    "  sentences[i] = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "badaad7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['govern announc new polici boost technolog sector .',\n",
       " 'stock market react posit increas foreign invest .',\n",
       " 'research develop new ai model medic diagnosi .',\n",
       " 'climat chang continu affect weather pattern worldwid .',\n",
       " 'startup increas adopt machin learn solut .',\n",
       " 'cybersecur threat increas digit transform .',\n",
       " 'educ system integr artifici intellig tool .',\n",
       " 'healthcar analyt improv patient treatment outcom .',\n",
       " 'renew energi invest grow rapid .',\n",
       " 'global economi adapt autom ai technolog .']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fe2dc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Taj Mahal is a beautiful monument located in India\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dbfefd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Taj', 'Mahal', 'is', 'a', 'beautiful', 'monument', 'located', 'in', 'India']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = nltk.word_tokenize(sentence)\n",
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d42933e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ENVY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7e4946d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Taj', 'NN')]\n",
      "[('Mahal', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('beautiful', 'NN')]\n",
      "[('monument', 'NN')]\n",
      "[('located', 'VBN')]\n",
      "[('in', 'IN')]\n",
      "[('India', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "#We will find pos tag\n",
    "for part in parts:\n",
    "  words = nltk.word_tokenize(part)\n",
    "  tagged = nltk.pos_tag(words)\n",
    "  print(tagged)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed47fe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ENVY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ENVY\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#named entity recognition\n",
    "sentence = \"Barack Obama was born in Hawaii. He was elected president in 2008.\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "tag_elements=nltk.pos_tag(words)\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7a74123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec,KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a113700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "vec_king = word_vectors['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19711f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,\n",
       "       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,\n",
       "       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,\n",
       "       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,\n",
       "        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,\n",
       "        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,\n",
       "       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,\n",
       "        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,\n",
       "        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,\n",
       "       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,\n",
       "        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,\n",
       "       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,\n",
       "        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,\n",
       "       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,\n",
       "       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,\n",
       "        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,\n",
       "       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_king"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e12bfeee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_king.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a736a1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.55541  ,  0.45894  ,  0.51851  , -0.045938 , -1.4064   ,\n",
       "        0.49701  , -0.085008 ,  0.63442  , -1.7949   , -0.31881  ,\n",
       "       -0.13673  , -1.1583   ,  0.45505  ,  0.21464  , -0.21751  ,\n",
       "       -0.21984  ,  0.60619  ,  0.55812  , -0.01031  ,  0.66228  ,\n",
       "        0.22206  ,  0.25498  ,  0.8452   , -0.72988  ,  0.26195  ,\n",
       "        0.26418  ,  0.22577  , -0.051338 ,  0.024459 ,  0.86389  ,\n",
       "       -0.35585  ,  0.48662  , -0.49752  , -0.44777  , -0.040533 ,\n",
       "       -0.18376  , -1.32     ,  0.54899  , -1.2289   , -0.22673  ,\n",
       "       -0.93431  ,  0.78923  ,  0.9565   , -1.3996   ,  1.0314   ,\n",
       "        0.39573  ,  0.7956   , -0.27184  ,  0.51776  , -1.0387   ,\n",
       "       -0.38121  ,  0.21772  ,  0.52486  ,  0.63307  , -0.21206  ,\n",
       "       -1.6741   , -1.3811   ,  0.079469 ,  0.46871  ,  0.29956  ,\n",
       "       -0.90023  , -0.16781  , -0.30873  ,  0.16586  ,  0.12141  ,\n",
       "        0.50219  ,  0.049859 ,  0.54896  ,  0.55576  , -0.14683  ,\n",
       "        0.55657  , -0.0060587,  0.25941  , -0.91918  ,  0.23     ,\n",
       "       -0.32992  ,  0.18277  ,  0.036235 , -0.71589  ,  0.22084  ,\n",
       "        0.3952   , -0.46155  ,  1.0515   , -0.58014  , -0.19766  ,\n",
       "       -0.39474  , -1.2366   , -0.37599  ,  0.070743 ,  0.93192  ,\n",
       "       -0.31744  ,  0.26108  , -0.23069  , -0.1247   ,  0.81297  ,\n",
       "        0.56446  ,  0.043594 ,  0.69141  ,  0.3543   ,  0.60907  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors['cricket']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12ac71f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rugby', 0.7827098369598389),\n",
       " ('twenty20', 0.7362942099571228),\n",
       " ('england', 0.7173773646354675),\n",
       " ('indies', 0.715782105922699),\n",
       " ('cricketers', 0.7132172584533691),\n",
       " ('football', 0.6659637093544006),\n",
       " ('zealand', 0.6606222987174988),\n",
       " ('matches', 0.6509107351303101),\n",
       " ('bowling', 0.6504763960838318),\n",
       " ('wc2003-wis', 0.642265260219574)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar('cricket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17090dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'m\", 0.8413287997245789),\n",
       " ('feel', 0.8132575750350952),\n",
       " (\"'re\", 0.8048083186149597),\n",
       " ('i', 0.7938276529312134),\n",
       " (\"'ll\", 0.7916273474693298),\n",
       " ('really', 0.7903971076011658),\n",
       " ('glad', 0.7833361029624939),\n",
       " ('good', 0.7821646332740784),\n",
       " ('we', 0.7808917164802551),\n",
       " ('sure', 0.7788466215133667)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar('happy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7703c393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.61933815)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.similarity(\"hockey\",\"sports\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speech-emotions-recognition-py3.12 (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
